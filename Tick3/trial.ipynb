{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [0 for _ in range(3)]\n",
    "sents = [1, -1, 0]\n",
    "for i in range(3):\n",
    "    labels[i] += 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [1, -1, 0]\n",
    "probs = [4, 7, 6]\n",
    "index = max(range(len(probs)), key=probs.__getitem__)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yangzejia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.sentiment_detection import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "s = '''The transition from beef to venison thus begins with today's venison burger Despite this novelty we chose the gnocchi carbonara which were regret to say did not meet expectations The sauce was too floury rather than cheesy and gnocchi appallingly overcooked to the point of becoming one with the sauce The cubes of ham were well seasoned and could be said to be the saving grace of the dish.\\\\\n",
    "Diners advised to go for the burger Served with a fascinating green tinged burger sauce, pink pickled onions as well as a large onion ring it certainly is abundant in toppings Sides of chips and slaw in addition make it by far the better choice Why the reviewer did not choose this remains a mystery that they attribute to a momentary cerebral malfunctioning they're thick that's why'''\n",
    "\n",
    "words = word_tokenize(s)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words\n",
    "filter_words = [word for word in words if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'re\",\n",
       " \"'s\",\n",
       " ',',\n",
       " 'abundant',\n",
       " 'addition',\n",
       " 'advised',\n",
       " 'appallingly',\n",
       " 'attribute',\n",
       " 'becoming',\n",
       " 'beef',\n",
       " 'begins',\n",
       " 'better',\n",
       " 'burger',\n",
       " 'carbonara',\n",
       " 'cerebral',\n",
       " 'certainly',\n",
       " 'cheesy',\n",
       " 'chips',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'chose',\n",
       " 'could',\n",
       " 'cubes',\n",
       " 'Despite',\n",
       " 'Diners',\n",
       " 'dish.\\\\',\n",
       " 'expectations',\n",
       " 'far',\n",
       " 'fascinating',\n",
       " 'floury',\n",
       " 'gnocchi',\n",
       " 'go',\n",
       " 'grace',\n",
       " 'green',\n",
       " 'ham',\n",
       " 'large',\n",
       " 'make',\n",
       " 'malfunctioning',\n",
       " 'meet',\n",
       " 'momentary',\n",
       " 'mystery',\n",
       " 'novelty',\n",
       " 'one',\n",
       " 'onion',\n",
       " 'onions',\n",
       " 'overcooked',\n",
       " 'pickled',\n",
       " 'pink',\n",
       " 'point',\n",
       " 'rather',\n",
       " 'regret',\n",
       " 'remains',\n",
       " 'reviewer',\n",
       " 'ring',\n",
       " 'said',\n",
       " 'sauce',\n",
       " 'saving',\n",
       " 'say',\n",
       " 'seasoned',\n",
       " 'Served',\n",
       " 'Sides',\n",
       " 'slaw',\n",
       " 'thick',\n",
       " 'thus',\n",
       " 'tinged',\n",
       " 'today',\n",
       " 'toppings',\n",
       " 'transition',\n",
       " 'venison',\n",
       " 'well']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(list(set(filter_words)), key=str.lower)\n",
    "with open(\"fw.txt\", 'w') as file:\n",
    "    for item in words:\n",
    "        file.write(\"%s\\n\" % item)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence , demonstrating removal stop words .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yangzejia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "# Sample text\n",
    "text = \"This is a sample sentence, demonstrating the removal of stop words.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Get the English stop words list from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Join the filtered tokens back into a single string\n",
    "filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "print(filtered_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
